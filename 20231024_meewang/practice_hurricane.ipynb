{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27a61d3",
   "metadata": {},
   "source": [
    "# 실습 - 허리케인 피해 지역 구분하기\n",
    "\n",
    "\n",
    "## 개요\n",
    "  - 허리케인이 지나간 위치의 위성 이미지를 보고, 피해 여부를 판단하는 모델을 만들어 봅시다.\n",
    "\n",
    "## 목표\n",
    "  - validation dataset 에 대해 95% 이상의 정확도를 가지는 것이 목표입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0fee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8920c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_data():\n",
    "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/satellitehurricaneimages.zip'\n",
    "    urllib.request.urlretrieve(url, 'satellitehurricaneimages.zip')\n",
    "    with zipfile.ZipFile('satellitehurricaneimages.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "\n",
    "download_and_extract_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a3fd2",
   "metadata": {},
   "source": [
    "# 1. 이미지 전처리\n",
    "\n",
    "  - image 전처리를 담당하는 함수를 만들어 봅시다.\n",
    "  - 간단한 normalize 만 수행해도 충분합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f13097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    # 함수 내용을 작성하세요.\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a6192",
   "metadata": {},
   "source": [
    "# 2. 데이터 준비\n",
    "\n",
    "  - 다운로드 받은 데이터를 읽어서 준비합니다.\n",
    "  - 위에서 정의한 preprocess 함수를 이용하여 전처리를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d74e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='train/',\n",
    "    image_size= (IMG_SIZE,IMG_SIZE)\n",
    "    , batch_size=BATCH_SIZE)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='validation/',\n",
    "    image_size= (IMG_SIZE,IMG_SIZE)\n",
    "    , batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(\n",
    "        tf.data.experimental.AUTOTUNE)\n",
    "val_ds = val_ds.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b2133",
   "metadata": {},
   "source": [
    "# 3. 모델 정의\n",
    "\n",
    "  - 모델 모양을 정의합니다.\n",
    "  - Input, Output 모양을 유의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33824a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # 함수 내용을 작성하세요.\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0050399",
   "metadata": {},
   "source": [
    "# 4. 모델 컴파일\n",
    "\n",
    "  - 학습 방법을 정의하세요\n",
    "  - loss, optimizer, metrics 를 올바르게 지정하면 충분합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846c32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    # 함수 내용을 작성하세요.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0c79f",
   "metadata": {},
   "source": [
    "# 5. 모델 학습\n",
    "\n",
    "  - 학습을 위한 epoch, batch_size 등을 설정하세요.\n",
    "  - 이 외에 사용하고 싶은 기법들은 자유롭게 도입하셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=5,\n",
    "        batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a477f2",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "- 이미지를 View 합니다.\n",
    "- model 로 predict 를 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb2e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Take one batch of images and labels\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='validation/',\n",
    "    image_size= (IMG_SIZE,IMG_SIZE)\n",
    "    , batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(val_ds.shuffle(buffer_size=10000)))\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "selected_image = images[data_index].numpy().astype(\"uint8\")\n",
    "selected_label = labels[data_index].numpy()\n",
    "\n",
    "image_to_predict = np.expand_dims(selected_image/255, axis=0)\n",
    "\n",
    "# Make a prediction\n",
    "predictions = model.predict(image_to_predict)\n",
    "predicted_label = int(predictions.round())\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(selected_image)\n",
    "class_names = val_ds.class_names\n",
    "plt.title(f\"True Label: {class_names[selected_label]}, Predicted: {class_names[predicted_label]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289c40db",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "- transfer learning 을 이용해서도 풀어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579cb971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자유롭게 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079174c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Hub version:\", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc33104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "model_name = \"mobilenet_v2_100_224\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
    "\n",
    "model_handle_map = {\n",
    "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
    "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
    "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
    "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
    "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
    "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
    "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
    "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
    "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
    "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
    "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
    "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
    "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
    "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
    "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
    "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
    "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
    "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
    "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
    "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
    "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
    "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
    "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
    "}\n",
    "\n",
    "model_image_size_map = {\n",
    "  \"efficientnetv2-s\": 384,\n",
    "  \"efficientnetv2-m\": 480,\n",
    "  \"efficientnetv2-l\": 480,\n",
    "  \"efficientnetv2-b0\": 224,\n",
    "  \"efficientnetv2-b1\": 240,\n",
    "  \"efficientnetv2-b2\": 260,\n",
    "  \"efficientnetv2-b3\": 300,\n",
    "  \"efficientnetv2-s-21k\": 384,\n",
    "  \"efficientnetv2-m-21k\": 480,\n",
    "  \"efficientnetv2-l-21k\": 480,\n",
    "  \"efficientnetv2-xl-21k\": 512,\n",
    "  \"efficientnetv2-b0-21k\": 224,\n",
    "  \"efficientnetv2-b1-21k\": 240,\n",
    "  \"efficientnetv2-b2-21k\": 260,\n",
    "  \"efficientnetv2-b3-21k\": 300,\n",
    "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
    "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
    "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
    "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
    "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
    "  \"efficientnetv2-b3-21k-ft1k\": 300,\n",
    "  \"efficientnet_b0\": 224,\n",
    "  \"efficientnet_b1\": 240,\n",
    "  \"efficientnet_b2\": 260,\n",
    "  \"efficientnet_b3\": 300,\n",
    "  \"efficientnet_b4\": 380,\n",
    "  \"efficientnet_b5\": 456,\n",
    "  \"efficientnet_b6\": 528,\n",
    "  \"efficientnet_b7\": 600,\n",
    "  \"inception_v3\": 299,\n",
    "  \"inception_resnet_v2\": 299,\n",
    "  \"nasnet_large\": 331,\n",
    "  \"pnasnet_large\": 331,\n",
    "}\n",
    "\n",
    "model_handle = model_handle_map.get(model_name)\n",
    "pixels = model_image_size_map.get(model_name, 224)\n",
    "\n",
    "print(f\"Selected model: {model_name} : {model_handle}\")\n",
    "\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(f\"Input size {IMAGE_SIZE}\")\n",
    "\n",
    "BATCH_SIZE = 16#@param {type:\"integer\"}\n",
    "do_fine_tuning = False #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building model with\", model_handle)\n",
    "model = tf.keras.Sequential([\n",
    "    # Explicitly define the input shape so the model can be properly\n",
    "    # loaded by the TFLiteConverter\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9),\n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa909a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='train/',\n",
    "    label_mode='categorical',\n",
    "    image_size= IMAGE_SIZE\n",
    "    , batch_size=BATCH_SIZE)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='validation/',\n",
    "    label_mode='categorical',\n",
    "    image_size= IMAGE_SIZE\n",
    "    , batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(\n",
    "        tf.data.experimental.AUTOTUNE)\n",
    "val_ds = val_ds.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab12eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    validation_data=val_ds).history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
